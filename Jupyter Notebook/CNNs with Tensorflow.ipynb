{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = joblib.load('../datasets/xtrain18.pkl')\n",
    "data_y = joblib.load('../datasets/ytrain.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_train, data_y, test_size = 0.25, random_state =123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 10, 100, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createWeight(size):\n",
    "    return tf.Variable(tf.truncated_normal(size, stddev=0.1))\n",
    "\n",
    "def createBias(size):\n",
    "    return tf.Variable(tf.constant(0.1,shape=size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w =10\n",
    "k =10\n",
    "graph_width = w*k\n",
    "graph_height = k\n",
    "nchannels = 18 \n",
    "batch_size = 32\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    graph_input = tf.placeholder(tf.float32,shape=(batch_size,graph_height,graph_width,nchannels))\n",
    "    targets = tf.placeholder(tf.float32,shape=(batch_size,2))\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    testX = tf.constant(test_x,dtype=tf.float32)\n",
    "    testy = tf.constant(test_y,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    W_conv1 = createWeight([k,k,nchannels,64])\n",
    "    b_conv1 = createBias([64])\n",
    "    \n",
    "    W_conv2 = createWeight([1,10,64,32])\n",
    "    b_conv2 = createBias([32])\n",
    "    \n",
    "    W = createWeight([1*(w-9)*32,128])\n",
    "    B = createBias([128])\n",
    "    \n",
    "    W_fc2 = createWeight([128,2])\n",
    "    b_fc2 = createBias([2])\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def model(data_input):\n",
    "        Y1 = tf.nn.relu(tf.nn.conv2d(data_input, W_conv1, strides=[1, w, w, 1], padding='VALID') + b_conv1)\n",
    "        #Y1_drop = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "        Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2)\n",
    "        #Y2_drop = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "        Y2_flat = tf.reshape(Y2, [-1, 1*(w-9)*32])\n",
    "        Y_fc1 = tf.nn.relu(tf.matmul(Y2_flat, W) + B)\n",
    "        Y_drop = tf.nn.dropout(Y_fc1, pkeep)\n",
    "\n",
    "        Y_conv = tf.matmul(Y_fc1, W_fc2) + b_fc2\n",
    "        return Y_conv\n",
    "\n",
    "    #train\n",
    "    logit_train = model(graph_input)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logit_train,labels = targets))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_train),1),tf.arg_max(targets,1)),tf.float32))\n",
    "    \n",
    "    #test\n",
    "    logit_test = model(testX)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_test),1),tf.arg_max(testy,1)),tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/10000 [00:00<37:13,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.4375])\n",
      "('test acc: ', [0.47599998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1067/10000 [00:03<00:22, 396.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.78125])\n",
      "('test acc: ', [0.55199999])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2042/10000 [00:05<00:19, 415.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.6875])\n",
      "('test acc: ', [0.58800006])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3052/10000 [00:08<00:20, 336.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.6875])\n",
      "('test acc: ', [0.57599998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4077/10000 [00:10<00:14, 413.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.625])\n",
      "('test acc: ', [0.57999998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5073/10000 [00:13<00:11, 418.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.34375])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6068/10000 [00:15<00:09, 429.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5625])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7059/10000 [00:17<00:07, 417.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.4375])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8052/10000 [00:20<00:04, 421.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9048/10000 [00:22<00:02, 420.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:24<00:00, 405.11it/s]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "for s in tqdm(range(num_steps)):\n",
    "    offset = (s*batch_size) % (train_x.shape[0]-batch_size)\n",
    "    fd = {graph_input:train_x[offset:offset+batch_size,:],\n",
    "         targets:train_y[offset:offset+batch_size,:]}\n",
    "    l,_ = sess.run([loss,opt],feed_dict=fd)\n",
    "    losses.append(l)\n",
    "    if s % 1000 == 0:\n",
    "        print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "        print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5625])\n",
      "('test acc: ', [0.5])\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w =10\n",
    "k =10\n",
    "graph_width = w*k\n",
    "graph_height = k\n",
    "nchannels = 18 \n",
    "batch_size = 32\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    graph_input = tf.placeholder(tf.float32,shape=(batch_size,graph_height,graph_width,nchannels))\n",
    "    targets = tf.placeholder(tf.float32,shape=(batch_size,2))\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    testX = tf.constant(test_x,dtype=tf.float32)\n",
    "    testy = tf.constant(test_y,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    W_conv1 = createWeight([k,k,nchannels,16])\n",
    "    b_conv1 = createBias([16])\n",
    "    \n",
    "    W_conv2 = createWeight([1,10,16,8])\n",
    "    b_conv2 = createBias([8])\n",
    "    \n",
    "    W = createWeight([1*(w-9)*8,32])\n",
    "    B = createBias([32])\n",
    "    \n",
    "    W_fc2 = createWeight([32,2])\n",
    "    b_fc2 = createBias([2])\n",
    "    \n",
    "    \n",
    "    #model\n",
    "    def model(data_input):\n",
    "        Y1 = tf.nn.relu(tf.nn.conv2d(data_input, W_conv1, strides=[1, w, w, 1], padding='VALID') + b_conv1)\n",
    "        #Y1_drop = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "        Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2)\n",
    "        #Y2_drop = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "        Y2_flat = tf.reshape(Y2, [-1, 1*(w-9)*8])\n",
    "        Y_fc1 = tf.nn.relu(tf.matmul(Y2_flat, W) + B)\n",
    "        Y_drop = tf.nn.dropout(Y_fc1, pkeep)\n",
    "\n",
    "        Y_conv = tf.matmul(Y_fc1, W_fc2) + b_fc2\n",
    "        return Y_conv\n",
    "\n",
    "    #train\n",
    "    logit_train = model(graph_input)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logit_train,labels = targets))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_train),1),tf.arg_max(targets,1)),tf.float32))\n",
    "    \n",
    "    #test\n",
    "    logit_test = model(testX)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_test),1),tf.arg_max(testy,1)),tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/10000 [00:00<00:33, 296.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.40625])\n",
      "('test acc: ', [0.44800004])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1068/10000 [00:02<00:19, 448.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.53125])\n",
      "('test acc: ', [0.528])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2080/10000 [00:04<00:17, 447.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.625])\n",
      "('test acc: ', [0.51200002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3046/10000 [00:06<00:16, 434.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.49600002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4060/10000 [00:09<00:13, 455.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.50400001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5075/10000 [00:11<00:11, 442.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.6875])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6082/10000 [00:13<00:08, 456.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.4375])\n",
      "('test acc: ', [0.50800002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7051/10000 [00:15<00:06, 458.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.59375])\n",
      "('test acc: ', [0.51199996])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8069/10000 [00:17<00:04, 457.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9074/10000 [00:20<00:02, 439.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:22<00:00, 448.07it/s]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "for s in tqdm(range(num_steps)):\n",
    "    offset = (s*batch_size) % (train_x.shape[0]-batch_size)\n",
    "    fd = {graph_input:train_x[offset:offset+batch_size,:],\n",
    "         targets:train_y[offset:offset+batch_size,:]}\n",
    "    l,_ = sess.run([loss,opt],feed_dict=fd)\n",
    "    losses.append(l)\n",
    "    if s % 1000 == 0:\n",
    "        print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "        print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5625])\n",
      "('test acc: ', [0.49999997])\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNNs With Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createWeightsBN(s):\n",
    "    \"\"\"\n",
    "    Creates weights for batch normalization layer\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    s: int\n",
    "        size of to be normalized\n",
    "    \"\"\"\n",
    "    gamma = tf.Variable(tf.truncated_normal([s]))\n",
    "    beta = tf.Variable(tf.ones([s]))\n",
    "    return [gamma,beta]\n",
    "\n",
    "def BN(x,variables,ri=[0,1,2],eps=.0001):\n",
    "    \"\"\"\n",
    "    Applies Batch Normalization\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    x: tensor\n",
    "        the data to normalize\n",
    "        \n",
    "    variables: [gamma,beta]\n",
    "        parameters to learn\n",
    "        \n",
    "    ri (optional): [int]\n",
    "        reduction indicies\n",
    "    eps (optional): float\n",
    "        small number to keep from dividing by zero\n",
    "    \"\"\"\n",
    "    gamma,beta = variables[0],variables[1]\n",
    "    \n",
    "    mu = tf.reduce_mean(x,ri,keep_dims=True)\n",
    "    sigma = tf.reduce_mean(tf.square(x-mu),ri,keep_dims=True)\n",
    "    x_hat = (x-mu)/(tf.sqrt(sigma+eps))\n",
    "    y = gamma*x_hat+beta\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w =10\n",
    "k =10\n",
    "graph_width = w*k\n",
    "graph_height = k\n",
    "nchannels = 18 \n",
    "batch_size = 50\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    graph_input = tf.placeholder(tf.float32,shape=(batch_size,graph_height,graph_width,nchannels))\n",
    "    targets = tf.placeholder(tf.float32,shape=(batch_size,2))\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    testX = tf.constant(test_x,dtype=tf.float32)\n",
    "    testy = tf.constant(test_y,dtype=tf.float32)\n",
    "    \n",
    "    bn_input = createWeightsBN(nchannels)\n",
    "    W_conv1 = createWeight([k,k,nchannels,12])\n",
    "    b_conv1 = createBias([12])\n",
    "    \n",
    "    bn_1 = createWeightsBN(12)\n",
    "    W_conv2 = createWeight([1,10,12,12])\n",
    "    b_conv2 = createBias([12])\n",
    "    \n",
    "    bn_2 = createWeightsBN(12)\n",
    "    W = createWeight([1*(w-9)*12,12])\n",
    "    B = createBias([12])\n",
    "    \n",
    "    bn_3 = createWeightsBN(12)\n",
    "    W_fc2 = createWeight([12,2])\n",
    "    b_fc2 = createBias([2])\n",
    "    bn_fc2 = createWeightsBN(2)\n",
    "    \n",
    "    #model\n",
    "    def model(data_input):\n",
    "        data_input_bn = BN(data_input,bn_input)\n",
    "        Y1 = tf.nn.relu(tf.nn.conv2d(data_input_bn, W_conv1, strides=[1, w, w, 1], padding='VALID') + b_conv1)\n",
    "        #Y1_drop = tf.nn.dropout(Y1, pkeep)\n",
    "        \n",
    "        y1_bn = BN(Y1,bn_1)\n",
    "        Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2)\n",
    "        #Y2_drop = tf.nn.dropout(Y2, pkeep)\n",
    "        \n",
    "        y2_bn = BN(Y2,bn_2)\n",
    "        Y2_flat = tf.reshape(Y2, [-1, 1*(w-9)*12])\n",
    "        Y_fc1 = tf.nn.relu(tf.matmul(Y2_flat, W) + B)\n",
    "        Y_drop = tf.nn.dropout(Y_fc1, pkeep)\n",
    "\n",
    "        Y_conv = tf.matmul(Y_fc1, W_fc2) + b_fc2\n",
    "        Y_conv_bn = BN(Y_conv,bn_fc2,ri=[0])\n",
    "        return Y_conv_bn\n",
    "\n",
    "    #train\n",
    "    logit_train = model(graph_input)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logit_train,labels = targets))\n",
    "    \n",
    "    opt = tf.train.RMSPropOptimizer(learning_rate=0.00001).minimize(loss)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_train),1),tf.arg_max(targets,1)),tf.float32))\n",
    "    \n",
    "    #test\n",
    "    logit_test = model(testX)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_test),1),tf.arg_max(testy,1)),tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/10000 [00:00<17:51,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.5])\n",
      "('test acc: ', [0.51999998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1041/10000 [00:03<00:32, 272.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.63999999])\n",
      "('test acc: ', [0.60000002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2051/10000 [00:07<00:29, 272.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.65999997])\n",
      "('test acc: ', [0.62399995])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3039/10000 [00:11<00:25, 267.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.75999999])\n",
      "('test acc: ', [0.63999999])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4047/10000 [00:14<00:22, 267.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.79999995])\n",
      "('test acc: ', [0.61199999])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5040/10000 [00:18<00:18, 272.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.77999997])\n",
      "('test acc: ', [0.616])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6052/10000 [00:22<00:14, 269.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.73999995])\n",
      "('test acc: ', [0.61599994])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7042/10000 [00:25<00:10, 275.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.75999999])\n",
      "('test acc: ', [0.62400001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8030/10000 [00:29<00:07, 276.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.71999997])\n",
      "('test acc: ', [0.616])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9056/10000 [00:33<00:03, 258.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.77999997])\n",
      "('test acc: ', [0.63599998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:37<00:00, 269.67it/s]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "for s in tqdm(range(num_steps)):\n",
    "    offset = (s*batch_size) % (train_x.shape[0]-batch_size)\n",
    "    fd = {graph_input:train_x[offset:offset+batch_size,:],\n",
    "         targets:train_y[offset:offset+batch_size,:]}\n",
    "    l,_ = sess.run([loss,opt],feed_dict=fd)\n",
    "    losses.append(l)\n",
    "    if s % 1000 == 0:\n",
    "        print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "        print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.82000005])\n",
      "('test acc: ', [0.63600004])\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Versi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w =10\n",
    "k =10\n",
    "graph_width = w*k\n",
    "graph_height = k\n",
    "nchannels = 18 \n",
    "batch_size = 50\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    graph_input = tf.placeholder(tf.float32,shape=(batch_size,graph_height,graph_width,nchannels))\n",
    "    targets = tf.placeholder(tf.float32,shape=(batch_size,2))\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "    testX = tf.constant(test_x,dtype=tf.float32)\n",
    "    testy = tf.constant(test_y,dtype=tf.float32)\n",
    "    \n",
    "    bn_input = createWeightsBN(nchannels)\n",
    "    W_conv1 = createWeight([k,k,nchannels,2])\n",
    "    b_conv1 = createBias([2])\n",
    "    \n",
    "#     bn_1 = createWeightsBN(16)\n",
    "#     W_conv2 = createWeight([1,5,16,8])\n",
    "#     b_conv2 = createBias([8])\n",
    "    \n",
    "    bn_2 = createWeightsBN(2)\n",
    "    W = createWeight([1*(w)*2,2])\n",
    "    B = createBias([2])\n",
    "    \n",
    "    bn_3 = createWeightsBN(2)\n",
    "    W_fc2 = createWeight([2,2])\n",
    "    b_fc2 = createBias([2])\n",
    "    bn_fc2 = createWeightsBN(2)\n",
    "    \n",
    "    #model\n",
    "    def model(data_input):\n",
    "        data_input_bn = BN(data_input,bn_input)\n",
    "        Y1 = tf.nn.relu(tf.nn.conv2d(data_input_bn, W_conv1, strides=[1, w, w, 1], padding='VALID') + b_conv1)\n",
    "        #Y1_drop = tf.nn.dropout(Y1, pkeep)\n",
    "        \n",
    "#         y1_bn = BN(Y1,bn_1)\n",
    "#         Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2)\n",
    "#         #Y2_drop = tf.nn.dropout(Y2, pkeep)\n",
    "        \n",
    "        y1_bn = BN(Y1,bn_2)\n",
    "        Y1_flat = tf.reshape(Y1, [-1, 1*(w)*2])\n",
    "        Y_fc1 = tf.nn.relu(tf.matmul(Y1_flat, W) + B)\n",
    "        Y_drop = tf.nn.dropout(Y_fc1, pkeep)\n",
    "\n",
    "        Y_conv = tf.matmul(Y_fc1, W_fc2) + b_fc2\n",
    "        Y_conv_bn = BN(Y_conv,bn_fc2,ri=[0])\n",
    "        return Y_conv_bn\n",
    "\n",
    "    #train\n",
    "    logit_train = model(graph_input)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logit_train,labels = targets))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=.0001).minimize(loss)\n",
    "    \n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_train),1),tf.arg_max(targets,1)),tf.float32))\n",
    "    \n",
    "    #test\n",
    "    logit_test = model(testX)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(tf.equal(\n",
    "        tf.arg_max(tf.nn.softmax(logit_test),1),tf.arg_max(testy,1)),tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session(graph=g)\n",
    "sess.run(init)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/10000 [00:00<14:10, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.44])\n",
      "('test acc: ', [0.57599998])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1025/10000 [00:04<00:39, 228.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.65999997])\n",
      "('test acc: ', [0.63199997])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2034/10000 [00:08<00:28, 282.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.79999995])\n",
      "('test acc: ', [0.66400003])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3018/10000 [00:12<00:30, 228.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.80000001])\n",
      "('test acc: ', [0.63200003])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4039/10000 [00:15<00:21, 277.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.94])\n",
      "('test acc: ', [0.62400001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5049/10000 [00:19<00:17, 280.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.94])\n",
      "('test acc: ', [0.62800002])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6030/10000 [00:22<00:14, 278.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.88])\n",
      "('test acc: ', [0.62400001])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7056/10000 [00:26<00:10, 278.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.94000006])\n",
      "('test acc: ', [0.62])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8039/10000 [00:30<00:06, 281.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.92000002])\n",
      "('test acc: ', [0.62])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9050/10000 [00:33<00:03, 277.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.95999998])\n",
      "('test acc: ', [0.60399997])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:37<00:00, 269.10it/s]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "for s in tqdm(range(num_steps)):\n",
    "    offset = (s*batch_size) % (train_x.shape[0]-batch_size)\n",
    "    fd = {graph_input:train_x[offset:offset+batch_size,:],\n",
    "         targets:train_y[offset:offset+batch_size,:]}\n",
    "    l,_ = sess.run([loss,opt],feed_dict=fd)\n",
    "    losses.append(l)\n",
    "    if s % 1000 == 0:\n",
    "        print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "        print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train acc: ', [0.94])\n",
      "('test acc: ', [0.616])\n"
     ]
    }
   ],
   "source": [
    "print('train acc: ',sess.run([train_accuracy],feed_dict=fd))\n",
    "print('test acc: ',sess.run([test_accuracy],feed_dict=fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
